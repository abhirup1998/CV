{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The load_data() function takes the main 'path' as input, and using nested for-loops accesses the images present inside the subfolders of the folder which the path points to.  The accessed images are read using opencv module and stored inside img_data array. The label array stores the names of the subfolders as labels.\n",
    "\n",
    "img_data is converted into a numpy array, and an extra _dimension_ is added(L 27) \n",
    "label_data uses the to_one_hot() function to create a suitable label array(L 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \n",
    "    sub_paths = os.listdir(path)\n",
    "\n",
    "    img_data = []\n",
    "    label_data = []\n",
    "    \n",
    "    c = 0\n",
    "    \n",
    "    for sub_path in sub_paths:\n",
    "        \n",
    "        img_paths = os.listdir(path + sub_path)\n",
    "        \n",
    "        for img_path in img_paths:\n",
    "            \n",
    "            img = cv2.imread(path + sub_path + '/' + img_path , 0)\n",
    "\n",
    "            label = int(sub_path)\n",
    "            \n",
    "            img_data.append(img)\n",
    "            label_data.append(label)\n",
    "            \n",
    "            c = c + 1\n",
    "            \n",
    "        print (sub_path)\n",
    "        \n",
    "    img_data = np.expand_dims(np.array(img_data),-1) #converting  to a numpy array\n",
    "    #label_data = tf.one_hot(label_data, 10).eval(session = sess) # one_hot representation\n",
    "    label_data = to_one_hot(label_data, 10)\n",
    "    \n",
    "    img_data, label_data = shuffle(img_data, label_data, random_state=0)\n",
    "    return img_data, label_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_one_hot(label_data, num_class):\n",
    "    num_sample = np.shape(label_data)[0]\n",
    "    temp = np.zeros([num_sample, num_class])\n",
    "    temp[np.arange(num_sample),label_data] = 1\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Network(Input): #input : [Batch_size, 32, 32, 1]\n",
    "    with tf.name_scope(\"Network\"):\n",
    "        conv1 = tf.layers.conv2d(Input, filters = 64, kernel_size = 3, strides = 1, activation = tf.nn.relu, name = 'conv1')\n",
    "        conv2 = tf.layers.conv2d(conv1, filters = 64, kernel_size = 3, strides = 1, activation = tf.nn.relu, name = 'conv2')\n",
    "        pool1 = tf.layers.max_pooling2d(conv2, pool_size = 2, strides = 2, name = 'pool1')\n",
    "\n",
    "        conv3 = tf.layers.conv2d(pool1, filters = 256, kernel_size = 3, strides = 1, activation = tf.nn.relu, name = 'conv3')\n",
    "        conv4 = tf.layers.conv2d(conv3, filters = 256, kernel_size = 3, strides = 1, activation = tf.nn.relu, name = 'conv4')\n",
    "        pool2 = tf.layers.max_pooling2d(conv4, pool_size = 2, strides = 2, name = 'pool2')\n",
    "\n",
    "        conv5 = tf.layers.conv2d(pool2, filters = 512, kernel_size = 3, strides = 1, activation = tf.nn.relu, name = 'conv5')\n",
    "\n",
    "        flat = tf.contrib.layers.flatten(conv5)\n",
    "        fc1 = tf.layers.dense(flat, units = 1024, activation = tf.nn.relu, name = 'fc1')\n",
    "        fc2 = tf.layers.dense(fc1, units = 10, activation = None, name = 'fc2')\n",
    "    \n",
    "    return fc2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_function(logit, Label):\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = logit, labels = Label))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Accuracy_Evaluate(prediction, Label):\n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Label, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    return correct_pred, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(train_data, train_label, \n",
    "#          no_of_epochs = 150000,\n",
    "         no_of_epochs = 1,\n",
    "         batchsize = 32):\n",
    "    \n",
    "    \n",
    "    Input = tf.placeholder(dtype = tf.float32, shape = [None, 28, 28, 1])\n",
    "    Label = tf.placeholder(dtype = tf.float32, shape = [None, 10])\n",
    "    \n",
    "    logit = Network(Input)\n",
    "    \n",
    "    prediction = tf.nn.softmax(logit)\n",
    "    \n",
    "    loss = loss_function(logit, Label)\n",
    "    \n",
    "    correct_pred, accuracy = Accuracy_Evaluate(prediction, Label)\n",
    "    \n",
    "    \n",
    "    optimiz = tf.train.GradientDescentOptimizer(learning_rate = 0.001).minimize(loss)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    " \n",
    "    \n",
    "    tf.summary.scalar('Loss_Value',loss)\n",
    "    tf.summary.scalar('Accuracy',accuracy)\n",
    "    \n",
    "    print('Stteing up summary op...')\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    print('Setting Up Saver...')\n",
    "    summary_writer = tf.summary.FileWriter('./log_dir/', sess.graph)\n",
    "    \n",
    "    itr = 0\n",
    "    for epoch in range(no_of_epochs):\n",
    "        \n",
    "        index = np.random.permutation(np.shape(train_data)[0])\n",
    "        train_data = train_data[index, :, :, :]\n",
    "        train_label = train_label[index, :]\n",
    "        \n",
    "        for idx in range(train_data.shape[0]//batchsize): \n",
    "            \n",
    "            batchx = train_data[idx*batchsize : (idx + 1)*batchsize , :, :, :]\n",
    "            batchy = train_label[idx*batchsize : (idx + 1)*batchsize, :]\n",
    "            \n",
    "            feed_dict = {Input : batchx , Label : batchy}\n",
    "            \n",
    "            _, train_loss, train_accuracy, summary_str = sess.run([optimiz, loss, accuracy, summary_op] , feed_dict )\n",
    "            summary_writer.add_summary(summary_str, itr)\n",
    "            itr = itr + 1\n",
    "            \n",
    "            if idx%10 == 0:\n",
    "                \n",
    "                print ('epoch : '+str(epoch)+' step : '+str(idx) + ' train_loss : '+str(train_loss) +\n",
    "                        ' train_accuracy : '+str(train_accuracy) \n",
    "            \n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "global sess\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "sess = tf.Session(config = config)\n",
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "6\n",
      "8\n",
      "1\n",
      "3\n",
      "2\n",
      "9\n",
      "4\n",
      "5\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "train_data, train_label = load_data('/home/abhirup/Desktop/Pycodes/Tf_Dl/trainingSet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 28, 28, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stteing up summary op...\n",
      "Setting Up Saver...\n",
      "epoch : 0 step : 0 train_loss : 2.66295 train_accuracy : 0.0625\n",
      "epoch : 0 step : 10 train_loss : 0.304401 train_accuracy : 0.28125\n",
      "epoch : 0 step : 20 train_loss : 0.266924 train_accuracy : 0.40625\n",
      "epoch : 0 step : 30 train_loss : 0.232729 train_accuracy : 0.53125\n",
      "epoch : 0 step : 40 train_loss : 0.207981 train_accuracy : 0.625\n",
      "epoch : 0 step : 50 train_loss : 0.141598 train_accuracy : 0.9375\n",
      "epoch : 0 step : 60 train_loss : 0.199252 train_accuracy : 0.625\n",
      "epoch : 0 step : 70 train_loss : 0.180698 train_accuracy : 0.6875\n",
      "epoch : 0 step : 80 train_loss : 0.127175 train_accuracy : 0.84375\n",
      "epoch : 0 step : 90 train_loss : 0.119343 train_accuracy : 0.84375\n",
      "epoch : 0 step : 100 train_loss : 0.124378 train_accuracy : 0.84375\n",
      "epoch : 0 step : 110 train_loss : 0.141581 train_accuracy : 0.78125\n",
      "epoch : 0 step : 120 train_loss : 0.111361 train_accuracy : 0.78125\n",
      "epoch : 0 step : 130 train_loss : 0.100952 train_accuracy : 0.9375\n",
      "epoch : 0 step : 140 train_loss : 0.120531 train_accuracy : 0.875\n",
      "epoch : 0 step : 150 train_loss : 0.157649 train_accuracy : 0.75\n",
      "epoch : 0 step : 160 train_loss : 0.101698 train_accuracy : 0.78125\n",
      "epoch : 0 step : 170 train_loss : 0.10401 train_accuracy : 0.84375\n",
      "epoch : 0 step : 180 train_loss : 0.0994377 train_accuracy : 0.8125\n",
      "epoch : 0 step : 190 train_loss : 0.111239 train_accuracy : 0.78125\n",
      "epoch : 0 step : 200 train_loss : 0.0989753 train_accuracy : 0.875\n",
      "epoch : 0 step : 210 train_loss : 0.105981 train_accuracy : 0.8125\n",
      "epoch : 0 step : 220 train_loss : 0.063839 train_accuracy : 0.96875\n",
      "epoch : 0 step : 230 train_loss : 0.0974715 train_accuracy : 0.875\n",
      "epoch : 0 step : 240 train_loss : 0.113995 train_accuracy : 0.8125\n",
      "epoch : 0 step : 250 train_loss : 0.0756744 train_accuracy : 0.9375\n",
      "epoch : 0 step : 260 train_loss : 0.110053 train_accuracy : 0.875\n",
      "epoch : 0 step : 270 train_loss : 0.0999893 train_accuracy : 0.90625\n",
      "epoch : 0 step : 280 train_loss : 0.12484 train_accuracy : 0.8125\n",
      "epoch : 0 step : 290 train_loss : 0.09869 train_accuracy : 0.8125\n",
      "epoch : 0 step : 300 train_loss : 0.0814682 train_accuracy : 0.9375\n",
      "epoch : 0 step : 310 train_loss : 0.0890707 train_accuracy : 0.96875\n",
      "epoch : 0 step : 320 train_loss : 0.0696386 train_accuracy : 0.90625\n",
      "epoch : 0 step : 330 train_loss : 0.0888949 train_accuracy : 0.875\n",
      "epoch : 0 step : 340 train_loss : 0.0950521 train_accuracy : 0.875\n",
      "epoch : 0 step : 350 train_loss : 0.12954 train_accuracy : 0.71875\n",
      "epoch : 0 step : 360 train_loss : 0.0779402 train_accuracy : 0.875\n",
      "epoch : 0 step : 370 train_loss : 0.0765459 train_accuracy : 0.84375\n",
      "epoch : 0 step : 380 train_loss : 0.0786373 train_accuracy : 0.90625\n",
      "epoch : 0 step : 390 train_loss : 0.0868927 train_accuracy : 0.84375\n",
      "epoch : 0 step : 400 train_loss : 0.0935994 train_accuracy : 0.90625\n",
      "epoch : 0 step : 410 train_loss : 0.0557909 train_accuracy : 0.9375\n",
      "epoch : 0 step : 420 train_loss : 0.0527763 train_accuracy : 0.96875\n",
      "epoch : 0 step : 430 train_loss : 0.0576475 train_accuracy : 0.90625\n",
      "epoch : 0 step : 440 train_loss : 0.052854 train_accuracy : 0.96875\n",
      "epoch : 0 step : 450 train_loss : 0.0435217 train_accuracy : 0.9375\n",
      "epoch : 0 step : 460 train_loss : 0.0572802 train_accuracy : 0.9375\n",
      "epoch : 0 step : 470 train_loss : 0.037562 train_accuracy : 0.9375\n",
      "epoch : 0 step : 480 train_loss : 0.0255793 train_accuracy : 1.0\n",
      "epoch : 0 step : 490 train_loss : 0.0373749 train_accuracy : 1.0\n",
      "epoch : 0 step : 500 train_loss : 0.0545669 train_accuracy : 0.90625\n",
      "epoch : 0 step : 510 train_loss : 0.0904036 train_accuracy : 0.90625\n",
      "epoch : 0 step : 520 train_loss : 0.0522873 train_accuracy : 0.9375\n",
      "epoch : 0 step : 530 train_loss : 0.0289135 train_accuracy : 0.96875\n",
      "epoch : 0 step : 540 train_loss : 0.0611808 train_accuracy : 0.90625\n",
      "epoch : 0 step : 550 train_loss : 0.0981164 train_accuracy : 0.78125\n",
      "epoch : 0 step : 560 train_loss : 0.0612398 train_accuracy : 0.90625\n",
      "epoch : 0 step : 570 train_loss : 0.0934138 train_accuracy : 0.84375\n",
      "epoch : 0 step : 580 train_loss : 0.0599355 train_accuracy : 0.9375\n",
      "epoch : 0 step : 590 train_loss : 0.0513024 train_accuracy : 0.90625\n",
      "epoch : 0 step : 600 train_loss : 0.0952098 train_accuracy : 0.875\n",
      "epoch : 0 step : 610 train_loss : 0.0491295 train_accuracy : 0.9375\n",
      "epoch : 0 step : 620 train_loss : 0.0665196 train_accuracy : 0.90625\n",
      "epoch : 0 step : 630 train_loss : 0.0663807 train_accuracy : 0.90625\n",
      "epoch : 0 step : 640 train_loss : 0.0668501 train_accuracy : 0.875\n",
      "epoch : 0 step : 650 train_loss : 0.0435199 train_accuracy : 1.0\n",
      "epoch : 0 step : 660 train_loss : 0.0633492 train_accuracy : 0.9375\n",
      "epoch : 0 step : 670 train_loss : 0.119828 train_accuracy : 0.8125\n",
      "epoch : 0 step : 680 train_loss : 0.0390395 train_accuracy : 0.90625\n",
      "epoch : 0 step : 690 train_loss : 0.0634029 train_accuracy : 0.90625\n",
      "epoch : 0 step : 700 train_loss : 0.0652751 train_accuracy : 0.90625\n",
      "epoch : 0 step : 710 train_loss : 0.0648229 train_accuracy : 0.9375\n",
      "epoch : 0 step : 720 train_loss : 0.0405086 train_accuracy : 1.0\n",
      "epoch : 0 step : 730 train_loss : 0.0655039 train_accuracy : 0.90625\n",
      "epoch : 0 step : 740 train_loss : 0.105525 train_accuracy : 0.875\n",
      "epoch : 0 step : 750 train_loss : 0.0473524 train_accuracy : 0.9375\n",
      "epoch : 0 step : 760 train_loss : 0.088862 train_accuracy : 0.875\n",
      "epoch : 0 step : 770 train_loss : 0.0386706 train_accuracy : 0.90625\n",
      "epoch : 0 step : 780 train_loss : 0.0407965 train_accuracy : 0.9375\n",
      "epoch : 0 step : 790 train_loss : 0.0493204 train_accuracy : 0.96875\n",
      "epoch : 0 step : 800 train_loss : 0.057346 train_accuracy : 0.90625\n",
      "epoch : 0 step : 810 train_loss : 0.058261 train_accuracy : 0.875\n",
      "epoch : 0 step : 820 train_loss : 0.042928 train_accuracy : 0.90625\n",
      "epoch : 0 step : 830 train_loss : 0.0506166 train_accuracy : 0.875\n",
      "epoch : 0 step : 840 train_loss : 0.0386204 train_accuracy : 0.9375\n",
      "epoch : 0 step : 850 train_loss : 0.0604136 train_accuracy : 0.90625\n",
      "epoch : 0 step : 860 train_loss : 0.0449157 train_accuracy : 0.96875\n",
      "epoch : 0 step : 870 train_loss : 0.0727267 train_accuracy : 0.78125\n",
      "epoch : 0 step : 880 train_loss : 0.0306953 train_accuracy : 0.96875\n",
      "epoch : 0 step : 890 train_loss : 0.0400246 train_accuracy : 0.9375\n",
      "epoch : 0 step : 900 train_loss : 0.0331731 train_accuracy : 0.96875\n",
      "epoch : 0 step : 910 train_loss : 0.0601349 train_accuracy : 0.875\n",
      "epoch : 0 step : 920 train_loss : 0.0481244 train_accuracy : 0.9375\n",
      "epoch : 0 step : 930 train_loss : 0.0676297 train_accuracy : 0.9375\n",
      "epoch : 0 step : 940 train_loss : 0.0339888 train_accuracy : 0.9375\n",
      "epoch : 0 step : 950 train_loss : 0.0520241 train_accuracy : 0.90625\n",
      "epoch : 0 step : 960 train_loss : 0.0294133 train_accuracy : 0.96875\n",
      "epoch : 0 step : 970 train_loss : 0.0574639 train_accuracy : 0.90625\n",
      "epoch : 0 step : 980 train_loss : 0.0650882 train_accuracy : 0.84375\n",
      "epoch : 0 step : 990 train_loss : 0.0417729 train_accuracy : 0.9375\n",
      "epoch : 0 step : 1000 train_loss : 0.0209936 train_accuracy : 0.96875\n",
      "epoch : 0 step : 1010 train_loss : 0.0869202 train_accuracy : 0.875\n",
      "epoch : 0 step : 1020 train_loss : 0.0412963 train_accuracy : 0.96875\n",
      "epoch : 0 step : 1030 train_loss : 0.0605007 train_accuracy : 0.9375\n",
      "epoch : 0 step : 1040 train_loss : 0.039463 train_accuracy : 0.96875\n",
      "epoch : 0 step : 1050 train_loss : 0.0317479 train_accuracy : 0.9375\n",
      "epoch : 0 step : 1060 train_loss : 0.0458092 train_accuracy : 0.9375\n",
      "epoch : 0 step : 1070 train_loss : 0.0367261 train_accuracy : 0.96875\n",
      "epoch : 0 step : 1080 train_loss : 0.0249917 train_accuracy : 1.0\n",
      "epoch : 0 step : 1090 train_loss : 0.0740068 train_accuracy : 0.90625\n",
      "epoch : 0 step : 1100 train_loss : 0.0490461 train_accuracy : 0.90625\n",
      "epoch : 0 step : 1110 train_loss : 0.0526165 train_accuracy : 0.96875\n",
      "epoch : 0 step : 1120 train_loss : 0.0668861 train_accuracy : 0.90625\n",
      "epoch : 0 step : 1130 train_loss : 0.0731525 train_accuracy : 0.90625\n",
      "epoch : 0 step : 1140 train_loss : 0.026445 train_accuracy : 1.0\n",
      "epoch : 0 step : 1150 train_loss : 0.0325614 train_accuracy : 0.96875\n",
      "epoch : 0 step : 1160 train_loss : 0.0127684 train_accuracy : 1.0\n",
      "epoch : 0 step : 1170 train_loss : 0.037774 train_accuracy : 0.96875\n",
      "epoch : 0 step : 1180 train_loss : 0.0347498 train_accuracy : 0.9375\n",
      "epoch : 0 step : 1190 train_loss : 0.0276567 train_accuracy : 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 step : 1200 train_loss : 0.0910231 train_accuracy : 0.84375\n",
      "epoch : 0 step : 1210 train_loss : 0.0594725 train_accuracy : 0.90625\n",
      "epoch : 0 step : 1220 train_loss : 0.0491435 train_accuracy : 0.96875\n",
      "epoch : 0 step : 1230 train_loss : 0.0499844 train_accuracy : 0.875\n",
      "epoch : 0 step : 1240 train_loss : 0.0237242 train_accuracy : 1.0\n",
      "epoch : 0 step : 1250 train_loss : 0.0176282 train_accuracy : 1.0\n",
      "epoch : 0 step : 1260 train_loss : 0.0310394 train_accuracy : 0.96875\n",
      "epoch : 0 step : 1270 train_loss : 0.0564827 train_accuracy : 0.90625\n",
      "epoch : 0 step : 1280 train_loss : 0.0510397 train_accuracy : 0.9375\n",
      "epoch : 0 step : 1290 train_loss : 0.0432021 train_accuracy : 0.96875\n",
      "epoch : 0 step : 1300 train_loss : 0.0202821 train_accuracy : 1.0\n",
      "epoch : 0 step : 1310 train_loss : 0.0438795 train_accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "main(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Resource\n",
    "# https://stackoverflow.com/questions/29831489/numpy-1-hot-array"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
